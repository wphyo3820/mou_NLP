{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing\n",
    "\n",
    "The libraries I'll use will consist of:\n",
    "- pandas for data structures/data analysis\n",
    "- numpy for simple scientific computing\n",
    "- sklearn for machine learning techniques\n",
    "- nltk for natural language processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing I did was to convert the pdf (since it was a one big file) into plaintext. \n",
    "\n",
    "I tried using PyPDF2 but that didn't work as I liked it to so I opted to use pdfminer instead to first convert it to plaintext.\n",
    "\n",
    "Here's the package repo: ```https://github.com/pdfminer/pdfminer.six```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I read in the file and tokenized it into sentences first. \n",
    "\n",
    "The sentences will then be futher pre-processed by checking whether each word in said sentence is alphanumeric and not a stopword.\n",
    "\n",
    "I also make sure the sentence will at least have a noun and a verb using POS (part of speech) tags using a universal tagset.\n",
    "\n",
    "Note: this process can be further improved by making sure a sentence also have a subject, adverb, adjective, but this will do so for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "with open(\"All_MOUs.txt\", \"r\") as f:\n",
    "    text= f.read()\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    cleaned_sentences = []\n",
    "    for s in sentences:\n",
    "        tokens = nltk.word_tokenize(s)\n",
    "        words = [word.lower() for word in tokens if word.isalpha() and word not in stopwords]\n",
    "        pos = nltk.pos_tag(words, tagset=\"universal\")\n",
    "        tags = set([tag[1] for tag in pos])\n",
    "        if set([\"NOUN\", \"VERB\"]).issubset(tags):\n",
    "            cleaned_sentences.append(\" \".join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I simply create a dataframe with all my cleaned sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.DataFrame(cleaned_sentences, columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>memorandum of understanding no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>for submission to the city council regarding the administrative unit this memorandum of understanding made entered day april amended day march by and between the city of los angeles and the engineers and architects association july june table of contents page general provisions unit membership list new employee information work access use city facilities bulletin boards actions employee relations board employment opportunities legislative agency shop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>grievances association security recognition parties memorandum understanding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>implementation memorandum understanding full understanding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>term calendar successor memorandum understanding</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                     text\n",
       "0  memorandum of understanding no                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "1  for submission to the city council regarding the administrative unit this memorandum of understanding made entered day april amended day march by and between the city of los angeles and the engineers and architects association july june table of contents page general provisions unit membership list new employee information work access use city facilities bulletin boards actions employee relations board employment opportunities legislative agency shop\n",
       "2  grievances association security recognition parties memorandum understanding                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "3  implementation memorandum understanding full understanding                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "4  term calendar successor memorandum understanding                                                                                                                                                                                                                                                                                                                                                                                                                      "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that my sentences are ready... I will use a term frequency-inverse document frequency (tf-idf) vectorizer and then convert the sentences into a tf-idf matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(text_df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<61549x8049 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 993135 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can now attempt to cluster my sentences (unsupervised learning) using kmeans with 5 clusters.\n",
    "\n",
    "Note: I attempted multiple clusters but 5 seem to be a good n to create our clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5, random_state=0).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cluster_5.pkl']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(kmeans, 'cluster_5.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then assign cluster labels back to my text_df dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df[\"cluster\"] = kmeans.labels_.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I try to identify which cluster correspond most to \"employee compensation\".\n",
    "\n",
    "Cluster 0 seems to relate to medical/leave/family.\n",
    "Cluster 1 seems to be pay/compensation (which is what we want!)\n",
    "Cluster 2 seems to be grievance related?\n",
    "Cluster 3 seems to be sick/vacation/time-off related.\n",
    "Cluster 4 seems to be legal/union/department related.\n",
    "\n",
    "You could make the argument that Cluster 3 could also be \"employee compensation\". But, I wasn't really sure by the question asking.. \"with regards to employee compensation\".\n",
    "\n",
    "I wanted to narrow my scope so I can actually define my target variable, hence I decided Cluster 1 will be my target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 words: \n",
      "leave\n",
      "family\n",
      "medical\n",
      "employee\n",
      "article\n",
      "taken\n",
      "health\n",
      "child\n",
      "care\n",
      "shall\n",
      "\n",
      "Cluster 1 words: \n",
      "hours\n",
      "shall\n",
      "time\n",
      "employee\n",
      "employees\n",
      "compensation\n",
      "day\n",
      "receive\n",
      "pay\n",
      "holiday\n",
      "\n",
      "Cluster 2 words: \n",
      "grievance\n",
      "within\n",
      "grievant\n",
      "days\n",
      "written\n",
      "procedure\n",
      "step\n",
      "shall\n",
      "the\n",
      "business\n",
      "\n",
      "Cluster 3 words: \n",
      "sick\n",
      "leave\n",
      "accrued\n",
      "use\n",
      "discretion\n",
      "used\n",
      "time\n",
      "employee\n",
      "subsection\n",
      "vacation\n",
      "\n",
      "Cluster 4 words: \n",
      "shall\n",
      "the\n",
      "employee\n",
      "city\n",
      "mou\n",
      "article\n",
      "management\n",
      "employees\n",
      "union\n",
      "department\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(5):\n",
    "    print(f\"Cluster {i} words: \")\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(f\"{terms[ind]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I designate cluster 1 to be my target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df[\"target\"] = text_df.apply(lambda row: True if row.cluster == 1 else False, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Modelling & Evaluation\n",
    "\n",
    "Now, we can try different models to implement binary classification.\n",
    "\n",
    "Below are the two models I found that works really well.\n",
    "I didn't want to overcomplicate things, opt for a lesser complexity/parsimonious models and as my professor used to say: \"Keep it simple stupid.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two binary classification models I found that works well are: Multinomial Naive Bayes, and Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf_NB = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])\n",
    "text_clf_LR = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', LogisticRegression(solver='lbfgs'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I test my model's accuracy using k-fold Cross-Validation with k = 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4541  150]\n",
      " [ 118 1346]]\n",
      "Accuracy:0.9564581640942323\n",
      "[[4534  144]\n",
      " [ 105 1372]]\n",
      "Accuracy:0.959545085296507\n",
      "[[4507  174]\n",
      " [  93 1381]]\n",
      "Accuracy:0.9566206336311942\n",
      "[[4557  154]\n",
      " [ 115 1329]]\n",
      "Accuracy:0.9562956945572705\n",
      "[[4582  158]\n",
      " [ 101 1314]]\n",
      "Accuracy:0.9579203899268887\n",
      "[[4567  172]\n",
      " [ 123 1293]]\n",
      "Accuracy:0.9520714865962632\n",
      "[[4564  150]\n",
      " [ 101 1340]]\n",
      "Accuracy:0.9592201462225832\n",
      "[[4594  154]\n",
      " [ 100 1307]]\n",
      "Accuracy:0.9587327376116979\n",
      "[[4573  149]\n",
      " [  95 1338]]\n",
      "Accuracy:0.960357432981316\n",
      "[[4527  134]\n",
      " [ 104 1389]]\n",
      "Accuracy:0.9613259668508287\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=10, shuffle = True, random_state = 100)\n",
    "for train_index, test_index in kf.split(text_df):\n",
    "    X_train, X_test = text_df.text[train_index], text_df.text[test_index]\n",
    "    y_train, y_test = text_df.target[train_index], text_df.target[test_index]\n",
    "    text_clf_NB = text_clf_NB.fit(X_train, y_train)\n",
    "    predicted = text_clf_NB.predict(X_test)\n",
    "    print(\"{}\\nAccuracy:{}\".format(confusion_matrix(y_test, predicted), np.mean(predicted == y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial Naive Bayes's accuracy hovers around ~95% and does pretty well. What about Logistic Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4671   20]\n",
      " [  40 1424]]\n",
      "Accuracy:0.9902518277822908\n",
      "[[4662   16]\n",
      " [  36 1441]]\n",
      "Accuracy:0.9915515840779854\n",
      "[[4658   23]\n",
      " [  26 1448]]\n",
      "Accuracy:0.9920389926888709\n",
      "[[4679   32]\n",
      " [  30 1414]]\n",
      "Accuracy:0.9899268887083672\n",
      "[[4715   25]\n",
      " [  33 1382]]\n",
      "Accuracy:0.9905767668562144\n",
      "[[4726   13]\n",
      " [  32 1384]]\n",
      "Accuracy:0.9926888708367181\n",
      "[[4687   27]\n",
      " [  38 1403]]\n",
      "Accuracy:0.9894394800974817\n",
      "[[4728   20]\n",
      " [  29 1378]]\n",
      "Accuracy:0.9920389926888709\n",
      "[[4708   14]\n",
      " [  34 1399]]\n",
      "Accuracy:0.9922014622258326\n",
      "[[4649   12]\n",
      " [  29 1464]]\n",
      "Accuracy:0.993337666558336\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=10, shuffle = True, random_state = 100)\n",
    "for train_index, test_index in kf.split(text_df):\n",
    "    X_train, X_test = text_df.text[train_index], text_df.text[test_index]\n",
    "    y_train, y_test = text_df.target[train_index], text_df.target[test_index]\n",
    "    text_clf_LR = text_clf_LR.fit(X_train, y_train)\n",
    "    predicted = text_clf_LR.predict(X_test)\n",
    "    print(\"{}\\nAccuracy:{}\".format(confusion_matrix(y_test, predicted), np.mean(predicted == y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!! The logistic regression model has a 99% accuracy and does much better than our previous Multinomial Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_model.pkl']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model = text_clf_LR.fit(text_df.text, text_df.target)\n",
    "joblib.dump(final_model, 'final_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we can use our logistic regression model to test on the .docx file.\n",
    "\n",
    "I will use a python library called docx to read in the word document file and clean it.\n",
    "\n",
    "This time however, I will keep note of the original sentences as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "\n",
    "doc = Document(\"MOU1_Compensation.docx\")\n",
    "paragraphs = [i.text.strip() for i in doc.paragraphs]\n",
    "sentences = nltk.sent_tokenize(\" \".join(paragraphs))\n",
    "cleaned_sentences = []\n",
    "original_sentences = []\n",
    "for s in sentences:\n",
    "    tokens = nltk.word_tokenize(s)\n",
    "    words = [word.lower() for word in tokens if word.isalpha() and word not in stopwords]\n",
    "    pos = nltk.pos_tag(words, tagset=\"universal\")\n",
    "    tags = set([tag[1] for tag in pos])\n",
    "    if set([\"NOUN\", \"VERB\"]).issubset(tags):\n",
    "        cleaned_sentences.append(\" \".join(words))\n",
    "        original_sentences.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame({\"original\": original_sentences, \"cleaned\": cleaned_sentences})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the logistic model predict, while also calculating its probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"predicted\"] = final_model.predict(test_df.cleaned)\n",
    "test_df[\"probability\"] = final_model.predict_proba(test_df.cleaned)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sort the dataframe by most likely to be \"employee compensation\" related based on our predicted probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = test_df.sort_values(\"probability\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are the top 5 most likely sentences that are related to \"employee compensation\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>predicted</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Overtime compensation shall be in time off at the rate of one and one-half (1½) hours for each hour of overtime worked or at the rate of one and one-half (1½) times the employee's regular rate of pay.</td>\n",
       "      <td>overtime compensation shall time rate one hours hour overtime worked rate one times employee regular rate pay</td>\n",
       "      <td>True</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>NOTE: \\tAn employee shall not receive court on-call overtime compensation and hour-for-hour overtime compensation for the same time period.</td>\n",
       "      <td>note an employee shall receive court overtime compensation overtime compensation time period</td>\n",
       "      <td>True</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>An off-duty employee shall receive a minimum of two hours overtime compensation for any court day he/she is subpoenaed to be on call or required to appear.</td>\n",
       "      <td>an employee shall receive minimum two hours overtime compensation court day subpoenaed call required appear</td>\n",
       "      <td>True</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Any employee receiving On-Call/Standby compensation for the same day shall not be eligible to receive compensation under this Article for that day.</td>\n",
       "      <td>any employee receiving compensation day shall eligible receive compensation article day</td>\n",
       "      <td>True</td>\n",
       "      <td>0.999996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>In addition, he/she shall receive hour-for-hour overtime compensation for each additional hour of actual court attendance in excess of two hours.</td>\n",
       "      <td>in addition shall receive overtime compensation additional hour actual court attendance excess two hours</td>\n",
       "      <td>True</td>\n",
       "      <td>0.999994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                     original                                                                                                        cleaned  predicted  probability\n",
       "25   Overtime compensation shall be in time off at the rate of one and one-half (1½) hours for each hour of overtime worked or at the rate of one and one-half (1½) times the employee's regular rate of pay.  overtime compensation shall time rate one hours hour overtime worked rate one times employee regular rate pay  True       1.000000   \n",
       "59   NOTE: \\tAn employee shall not receive court on-call overtime compensation and hour-for-hour overtime compensation for the same time period.                                                               note an employee shall receive court overtime compensation overtime compensation time period                   True       1.000000   \n",
       "56   An off-duty employee shall receive a minimum of two hours overtime compensation for any court day he/she is subpoenaed to be on call or required to appear.                                               an employee shall receive minimum two hours overtime compensation court day subpoenaed call required appear    True       0.999999   \n",
       "104  Any employee receiving On-Call/Standby compensation for the same day shall not be eligible to receive compensation under this Article for that day.                                                       any employee receiving compensation day shall eligible receive compensation article day                        True       0.999996   \n",
       "61   In addition, he/she shall receive hour-for-hour overtime compensation for each additional hour of actual court attendance in excess of two hours.                                                         in addition shall receive overtime compensation additional hour actual court attendance excess two hours       True       0.999994   "
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if one wishes so... we can output the predicted \"employee compensation\" related sentences like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_doc = Document()\n",
    "for p in test_df[test_df.predicted == True].original:\n",
    "    out_doc.add_paragraph(p)\n",
    "out_doc.save(\"output.docx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! I can probably make this into a command line script that takes any docx/text files as input using click or arg_parse (if that's what required). I wasn't really sure.\n",
    "\n",
    "Please let me know if there is any improvements I can make to my approach or any valuable lessons!\n",
    "\n",
    "Thanks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
